---
title: "Lab 9: Regression"
author: "Dr. Kyle Teller"
date: "2023-07-16"
output: 
  html_document: 
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE,message=FALSE)

```

# Introduction

We will be working with data that are sets of ordered pairs of numbers. The ordered pairs are labeled $(x_i,y_i )$ for $i = 1$ to $n$. The $y$'s are the called the response (dependent) variable and the $x$'s are called the control (independent) variable.

**Simple Linear Regression** is widely used to approach the possible relationship between $x$ and $y$, as long as the following assumptions are satisfied.

**Assumptions for Simple Linear Regression Model:**

1.  Each $y_i$ is related to $x_i$ via the equation $y_i=\beta_0+\beta_1x_i+\epsilon_i$;
2.  The $x_i$'s are given: i.e., the $x_i$'s are not random variables;
3.  The $\epsilon_i$'s are **independent normal** random variables with mean 0 and standard deviation $\sigma$;
4.  $\beta_0, \beta_1$, and $\sigma$ are unknown **population parameters** to be estimated from the **sample** data.

When using simple linear regression, there are two major tasks.

I. Determine whether or not it is reasonable to assume our data was generated by the simple linear regression model.

II\. If we answer yes to I., then we need to estimate the numbers $\beta_0, \beta_1$, and $\sigma$ and use our estimates to answer questions, make decisions, etc.

The estimates for $\beta_0, \beta_1$, and $\sigma$ are usually called $\widehat{\beta_0}, \widehat{\beta_1}$, and $\widehat{\sigma}$. And once we have found $\widehat{\beta_0}, \widehat{\beta_1}$, and $\widehat{\sigma}$, then given a value of $x$, we can use $\widehat{\beta_0}, \widehat{\beta_1}$, and $\widehat{\sigma}$ to estimate the mean value of the response variable $y$. This estimate is called $\widehat{y}$ and is calculated as follows:

$$
\widehat{y}=\widehat{\beta_0}+\widehat{\beta_1}x.
$$

The **residuals** $\widehat{e_i}$ are given by the following equation

$$
\widehat{e_i}=y_i-\widehat{y_i}=y_i-(\widehat{\beta_0}+\widehat{\beta_1}x_i)
$$

and are central in analyzing the simple linear regression model.   Analyzing the residuals can lead to further evidence the assumptions for simple linear regression have or have not been met.  In addition, if the assumptions for the simple linear regression model are satisfied, the residuals carry information about how reliable our estimates might be.

A straightforward way to begin addressing the assumptions for simple linear regression is to scatter plot the ordered pairs and display the line $\widehat{y}=\widehat{\beta_0}+\widehat{\beta_1}x$ on the same graph. If the scatter plot of the data points seems to scatter non randomly about the line $\widehat{y}=\widehat{\beta_0}+\widehat{\beta_1}x$, then it's likely that one or more of the assumptions for simple linear regression are not satisfied and something else possibly should be done. If the scatter plot looks reasonable then we move forward and do a more thorough analysis of the ordered pairs.

In today's lab, we will first work will simulated data to understand the elementary processes of simple linear regression, then we will work on real word data, and finally we will consider some special examples about regression.

# Part I: Working with Simulated Data

In part I, we first pre-set the relationship between $x$'s and $y$'s (step 0), then simulate sample pairs $(x_i,y_i)$ based on that pre-set relationship (step 1). After the sample pairs are obtained, we "pretend" that we don't know that relationship, and wish to approach/estimate that relationship based on the sample data (step 2).

**Step 0**: We first define that the true relationship between $x$ and $y$ is given by

$$
y_i=2+4x_i+\epsilon_i
$$

**Step 1.1**: Define $x_i$: recall assumption 2, $x_i$ should be given, not random. Here we define $x_i$'s as numbers from 0 through 2 in steps of 0.1. To do that, we use the seq(from, to, by) function. This is the x in part I, we use the object x1 to store the results

```{r}
x1 <- seq(from=0,to=2,by=0.1)
x1
```

**Step 1.2**: Define $\epsilon_i$: recall assumption 3, $\epsilon_i$ should be independent normal random variables with mean 0 and fixed standard deviation (here we set $\sigma=1$). To simulate such random variables, we use function rnorm(number of random variable to generate, mean, standard deviation):

```{r}
eps1 <- rnorm(21,0,1)
```

**Step 1.3**: Simulate $y_i$: This is our first model in part I, we store the results in object y1_1

```{r}
y1_1 <- 2+4*x1+eps1
```

Up to this point, we obtain the 21 sample pairs of $(x_i,y_i)$. Now, let's forget the true relationship and try to approach that relationship. If we assume that the relationship is linear, we use **simple linear regression analysis** to do that.

**Step 2.1**: Obtain the scatter plot of $(x_i,y_i)$

```{r}
plot(y1_1~x1)
title(main="Scatter plot - Model 1 - Your Name")
```

Does the above scatter plot shows an approximately linear pattern? If yes, we can go to the next sub-step

**Step 2.2**: Simple Linear Regression Analysis

```{r}
mod1 <- lm(y1_1~x1)
summary(mod1)
plot(mod1)
```

How to interpret the analysis results?

Finally, let's plot the scatter plot with regression line together

```{r}
plot(y1_1~x1)
abline(mod1)
```

What is your observation?

## Assignment 1.1

Now mimic the above processes, use the same x1 and eps1, define the second y using y1_2\<-3-2\*x1+eps1, then obtain the scatter plot, check if the relationship between y1_2 and x1 is approximately linear, if yes, run the simple linear regression analysis (store the result in object mod2), interpret the results, and finally plot the scatter plot together with the regression line.

Remark: When you create the plots, make sure you add the title with your name.

R codes:

```{r}
y1_2<-3-2*x1+eps1
plot(y1_2~x1)
title(main="Scatter plot - Model 2 - James")
mod2 <- lm(y1_2~x1)
summary(mod2)
plot(mod2)
abline(mod2)

```

Interpretation:

```         
There are 3 outliers, and the line of best fit doesn't come near crossing any points after 0.10
```

**End of Assignment 1.1**

Next, we will explore the cases when the assumptions 1 or 3 for the linear regression analysis are not satisfied.

Assumption 1 says that the relationship between $x$ and $y$ are linear. So we define our mod 3 as

$$
y_i=2+4x_i^3+\epsilon_i
$$

where $\epsilon_i$ still satisfies assumption 3, independent normal random variables with mean 0 and standard deviation 1.

## Assignment 1.2

Now mimic the above processes, use the same x1 and eps1, define the second y using y1_3\<-2+4\*x1\^3+eps1, then obtain the scatter plot, check if the relationship between y1_3 and x1 is approximately linear, still run the simple linear regression analysis (store the result in object mod3), but check how reliable the result is, and finally plot the scatter plot together with the regression line.

Remark: When you create the plots, make sure you add the title with your name.

R codes:

```{r}
y1_3<-2+4*x1^3+eps1
plot(y1_3~x1)
title(main="Scatter plot - Model 3 - James")
mod3 <- lm(y1_3~x1)
summary(mod3)
plot(mod3)
abline(mod3)
```

Interpretation:

```         
This model is a lot less sparce than model 2. The relationship between the two is appox. linear. More linear than model 2.
```

**End of Assignment 1.2**

What if the assumption 3 is not satisfied?

Assumption 3 says that the error term $\epsilon$ should be independent normal random variable with mean 0 and fixed standard deviation, so we define our model 4 as

$$
y_i=2+4x_i+\epsilon_i^*
$$

where $\epsilon_i^*$ follows a skewed distribution but still with mean 0.

To do that, use

```{r}
eps2 <- rlnorm(21,0,1)-exp(1/2)
hist(eps2)
mean(eps2)
```

## Assignment 1.3

Now mimic the above processes, use the same x1 and eps1, define the second y using y1_4\<-2+4\*x1+eps2, then obtain the scatter plot, check if the relationship between y1_4 and x1 is approximately linear, still run the simple linear regression analysis (store the result in object mod3), but check how reliable the result is, and finally plot the scatter plot together with the regression line.

Remark: When you create the plots, make sure you add the title with your name.

R codes:

```{r}
y1_4<-2+4*x1+eps2
plot(y1_4~x1)
title(main="Scatter plot - Model 4 - James")
mod4 <- lm(y1_4~x1)
summary(mod4)
plot(mod4)
abline(mod4)
```

Interpretation:

```         
There seems to only be one outlier on the scatterplot and the result is very reliable. A very linear relationship.
```

**End of Assignment 1.3**

**Comment**: In practice, when non-linearity or non-normal pattern are observed, we would not do a simple linear regression analysis on that data set. Instead we would probably try to transform or preprocess the pairs in some way to make them look more like a straight line and then do a simple regression or we might try a non-linear regression.

# Part II: Example with Real-World Data

Read the data file "TestScore.csv". This data set contains MATH 155 midterms, homework, final exam and course average. Call the object of this data file as TestScore

R code for reading the data:

```{r}
TestScore <- read.csv("TestScore.csv")

```

This data is a combined (deidentified) test, homework, final exam and course average data from two sections of Math 155 taught in Fall 2017.

## Assignment 2

1.  Create scatter plots of all variables against course average (test 1 vs. course average, test 2 vs. course average etc. a total of 6 graphs). Which response variable is the strongest in predicting course average in your opinion? How do you decide?

    R codes:

    ```{r}
    TestScore
    first<-plot(TestScore$T1~TestScore$CourseAverage)
    second<-plot(TestScore$T2~TestScore$CourseAverage)
    third<-plot(TestScore$T3~TestScore$CourseAverage)
    fourth<-plot(TestScore$Tave~TestScore$CourseAverage)
    fith<-plot(TestScore$HW~TestScore$CourseAverage)
    sixth<-(TestScore$FinalExam~TestScore$CourseAverage)
    ```

    Interpretation:

    ```         
    Tave, and I chose Tave because it would most likely have the tightest line of best fit, being highly related to course average.
    ```

2.  Do a complete regression analysis on all 6 pairs. Report 6 regression line equations and R^2^ values. Which explanatory variable is the best in predicting course average?

    R codes:

    ```{r}
    first <- lm(TestScore$T1~TestScore$CourseAverage)
    summary(first)
    plot(first)
    abline(first)

    second <- lm(TestScore$T2~TestScore$CourseAverage)
    summary(second)
    plot(second)
    abline(second)

    third <- lm(TestScore$T3~TestScore$CourseAverage)
    summary(third)
    plot(third)
    abline(third)

    fourth <- lm(TestScore$Tave~TestScore$CourseAverage)
    summary(fourth)
    plot(fourth)
    abline(fourth)

    fith <- lm(TestScore$HW~TestScore$CourseAverage)
    summary(fith)
    plot(fith)
    abline(fith)

    sixth <- lm(TestScore$FinalExam~TestScore$CourseAverage)
    summary(sixth)
    plot(sixth)
    abline(sixth)
    ```

    Interpretation:

    ```         
    test1 because its line fits best.
    ```

3.  Notice how some final exam scores are zeros. These students didn't show up on the final exam. Do you think those zeros can affect the outcome of regression analysis? Explain.

    R codes:

    ```{r}


    ```

    Interpretation:

    ```         
    Yes, because they are outliers
    ```

4.  Now remove pairs of final exam -- course average for those that have zeros in final exam. To do that, you can use the code as

    ```{r}
    TestScore2<-subset(TestScore, FinalExam!=0)
    ```

    Then rerun the regression. Did it change the outcome of the regression analysis? Did it make it stronger or weaker? How do you know?

    R codes:

    ```{r}
    sixth <- lm(TestScore2$FinalExam~TestScore2$CourseAverage)
    sixth
    summary(sixth)
    plot(sixth)
    abline(sixth)
    ```

    Interpretation:

    ```         
    It changed making it weaker. Going from r^2 of .88 to .73
    ```

5.  Out of all regression analyses you made, choose the best predictor. Do residuals satisfy necessary conditions? Is the regression analysis valid? Can it be used for estimation and prediction?

    Interpretation:

    ```         
    Tave is the most valid and can be used for esitmation and prediction.
    ```

**End of Assignment 2**

# Part III: Special Cases

A statistician named Frank Anscombe made up this data, to illustrate a point about regression.

## Assignment 3

Read the data from data file "FA.csv", store it in an object called "FA"

Run the regression analysis for Y1 on X, Y2 on X, Y3 on X, and Y4 on X4. Compare the four sets of regression output. What do they all have in common? List all of the commonalities.

If you only looked at the regression output, would you think these data sets were alike?

Produce four scatter plots for the same four pairs of columns\--put them all on the same graph but in different panels. What was Anscombe trying to illustrate with this example?

R codes:

thr \<- lm(FA\$Y3\~FA\$X)

summary(thr)

plot(thr)

abline(thr)

```{r}
FA <- read.csv("FA.csv")
FA


sixth <- lm(FA$Y1~FA$X)
summary(sixth)
plot(sixth)
abline(sixth)

bruh <- lm(FA$Y2~FA$X)
summary(bruh)
plot(bruh)
abline(bruh)

thr <- lm(FA$Y3~FA$X)
summary(thr)
plot(thr)
abline(thr)

fr <- lm(FA$Y4~FA$X4)
summary(fr)
plot(fr)
abline(fr)


par(mfrow = c(2, 2))

plot(FA$Y1~FA$X)
plot(FA$Y2~FA$X)
plot(FA$Y3~FA$X)
plot(FA$Y4~FA$X4)
```

Interpretation:

```         
They all have a r^2 of .62. If I only looked at the outputs I wouldn't think they have anything in commmon. Anscomb wanted to show that even with varying in data spread the summary of the data can be very similar.
```

**End of Assignment 3**
